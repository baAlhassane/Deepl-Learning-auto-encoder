{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group:\n",
    "\n",
    "- Student 1:Alhassane BA\n",
    "- Student 2:\n",
    "- Student 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical classes\n",
    "\n",
    "All exercices will be in Python. It is important that you keep track of exercices and structure you code correctly (e.g. create funcions that you can re-use later)\n",
    "\n",
    "We will use Jupyter notebooks (formerly known as IPython). You can read the following courses for help:\n",
    "* Python and numpy: http://cs231n.github.io/python-numpy-tutorial/\n",
    "* Jupyter / IPython : http://cs231n.github.io/ipython-tutorial/\n",
    "\n",
    "To run this notebook:\n",
    "* create a directory somewhere on your filesystem\n",
    "* download the .ipynb from the course website: http://teaching.caio-corro.fr/2020-2021/OPT4/docs/lab/\n",
    "* move the .ipynb into the directory\n",
    "* from a terminal:\n",
    "    * cd /directory/path\n",
    "    * jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "You need to install the following libraries: numpy, scikit-learn and matplotlib.\n",
    "Please refer to the internet for instruction if you don't know how to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy is one of the most popular numerical computation library in Python.\n",
    "For this lab exercise we are mainly interested in tensor computation.\n",
    "\n",
    "It is really important that you take time to understand how Numpy works. A short tutorial is available here: https://cs231n.github.io/python-numpy-tutorial/\n",
    "\n",
    "Take time to do a few test, understand the different operation, the different between in-place and out-of-place operations, etc.\n",
    "The most important resource you **must** use is the numpy documentation.\n",
    "As we usually say in computer science: Read The F*cking Manual https://numpy.org/doc/stable/reference/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the tensor:  (2, 5)\n",
      "Content:\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# create a 2D tensor of shape (2, 5) full of zeros\n",
    "# by default the tensor will contain elements of type float\n",
    "t = np.zeros((2, 5))\n",
    "print(\"Shape of the tensor: \", t.shape)\n",
    "print(\"Content:\")\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "(1, 10)\n",
      "(10, 1)\n",
      "(2, 5)\n"
     ]
    }
   ],
   "source": [
    "# You can reshape tensors\n",
    "# When you reshape a tensor, it does not change the data order in the underlying memory.\n",
    "# By default, this is the \"C array order\", also called the row-major format.\n",
    "# If you don't know about this, check the wikipedia page:\n",
    "# https://en.wikipedia.org/wiki/Row-_and_column-major_order\n",
    "\n",
    "# for example, the following operation will reshape t as a vector with ten elements\n",
    "t = t.reshape(-1) # -1 means \"put every there\"\n",
    "print(t.shape)\n",
    "\n",
    "# here instead of having a vector we build a tensor with a single row with ten elements\n",
    "t = t.reshape(1, -1)\n",
    "# of cours we could have done t = t.reshape(1, 10)\n",
    "print(t.shape)\n",
    "\n",
    "# here a tensor with a single column with ten elements\n",
    "t = t.reshape(-1, 1)\n",
    "# of cours we could have done t = t.reshape(10, 1)\n",
    "print(t.shape)\n",
    "\n",
    "# reshape into the original shape\n",
    "t = t.reshape(2, -1)\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3]\n",
      "[[0 1]\n",
      " [2 3]]\n",
      "[[0 2]\n",
      " [1 3]]\n"
     ]
    }
   ],
   "source": [
    "# this creates a vector with values from 0 to 3 (not included)\n",
    "t = np.arange(4)\n",
    "print(t)\n",
    "\n",
    "# reshape\n",
    "t = t.reshape((2, 2))\n",
    "print(t)\n",
    "\n",
    "# .T returns the transposed tensor\n",
    "print(t.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New content:\n",
      "[[0 1]\n",
      " [1 3]]\n"
     ]
    }
   ],
   "source": [
    "# set the first element of the second row to one and display the new data\n",
    "# this is an in-place operation: it directly modifes the tensor memory\n",
    "t[1, 0] = 1.\n",
    "print(\"New content:\")\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor:\n",
      "[[0 1]\n",
      " [1 3]]\n",
      "New tensor:\n",
      "[[0 2]\n",
      " [2 6]]\n"
     ]
    }
   ],
   "source": [
    "# multiply the content of the tensor by two\n",
    "# this is an out-of-place operation: it does not modify the tensory memory but creates a new one\n",
    "t2 = 2 * t\n",
    "\n",
    "print(\"Original tensor:\")\n",
    "print(t)\n",
    "print(\"New tensor:\")\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New content:\n",
      "[[0 2]\n",
      " [2 6]]\n"
     ]
    }
   ],
   "source": [
    "# do the same thing but in-place\n",
    "t *= 2\n",
    "print(\"New content:\")\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      "[[0 1]\n",
      " [2 3]]\n",
      "b:\n",
      "[[2 2]\n",
      " [2 2]]\n",
      "\n",
      "a * b:\n",
      "[[0 2]\n",
      " [4 6]]\n",
      "\n",
      "a @ b:\n",
      "[[ 2  2]\n",
      " [10 10]]\n"
     ]
    }
   ],
   "source": [
    "# There are two multiplication operators:\n",
    "# * is the element wise multiplication operator (also called the Hadamard product)\n",
    "# @ is the matrix multiplication operator\n",
    "\n",
    "a = np.arange(4).reshape(2, 2)\n",
    "# one_like create a tensor with the same properties (i.e. type and shape) than the argument\n",
    "# but filled with ones\n",
    "b = 2 * np.ones_like(a) \n",
    "\n",
    "print(\"a:\")\n",
    "print(a)\n",
    "print(\"b:\")\n",
    "print(b)\n",
    "print()\n",
    "\n",
    "# element wise multiplication\n",
    "c = a * b\n",
    "print(\"a * b:\")\n",
    "print(c)\n",
    "print()\n",
    "\n",
    "# matrix multiplication\n",
    "c = a @ b\n",
    "print(\"a @ b:\")\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      "[[0 1]\n",
      " [2 3]]\n",
      "\n",
      "first row of a:\n",
      "[0 1]\n",
      "\n",
      "first column of a:\n",
      "[0 2]\n"
     ]
    }
   ],
   "source": [
    "# you can easily retrieve one row or one column of a tensor\n",
    "print(\"a:\")\n",
    "print(a)\n",
    "print()\n",
    "\n",
    "print(\"first row of a:\")\n",
    "print(a[0])\n",
    "print()\n",
    "\n",
    "print(\"first column of a:\")\n",
    "print(a[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      "[[0 1]\n",
      " [2 3]]\n",
      "\n",
      "after update:\n",
      "[[ 0 10]\n",
      " [ 2 10]]\n"
     ]
    }
   ],
   "source": [
    "# the same approach can be used to update the data in-place\n",
    "print(\"a:\")\n",
    "print(a)\n",
    "print()\n",
    "\n",
    "# set the second colums elements to 10\n",
    "a[:, 1] = 10.\n",
    "print(\"after update:\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most important feature you have to understand is **broadcasting**.\n",
    "You can read the following article to understand operation broadcasting: https://numpy.org/devdocs/user/theory.broadcasting.html\n",
    "\n",
    "It is a very important concept that is really helpful in numpy and other numerical computation library.\n",
    "The documentation often explain of broadcasting is implemented for a given operation, check for example the matrix multiplication page: https://numpy.org/doc/stable/reference/generated/numpy.matmul.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: \n",
      "[[0 1 2]\n",
      " [3 4 5]]\n",
      "\n",
      "new tensor:\n",
      "[[ 0  2  4]\n",
      " [12 16 20]]\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(6).reshape(2, -1)\n",
    "print(\"a: \")\n",
    "print(a)\n",
    "print()\n",
    "\n",
    "# we will multpliy the first row by 2 and the second row by 4 by using operation broadcasting\n",
    "# np.array can be used to create a tensor from python data\n",
    "b = np.array([2, 4]).reshape((2, 1))\n",
    "c = a * b\n",
    "\n",
    "print(\"new tensor:\")\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network: first experiments with a linear model\n",
    "\n",
    "In this first lab exercise we will code a neural network using numpy, without a neural network library.\n",
    "Next week, the lab exercise will be to extend this program with hidden layers and activation functions.\n",
    "\n",
    "The task is digit recognition: the neural network has to predict which digit in $\\{0...9\\}$ is written in the input picture. We will use the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset, a standard benchmark in machine learning.\n",
    "\n",
    "The model is a simple linear  classifier $o = \\operatorname{softmax}(Wx + b)$ where:\n",
    "* $x$ is an input image that is represented as a column vector, each value being the \"color\" of a pixel\n",
    "* $W$ and $b$ are the parameters of the classifier\n",
    "* $\\operatorname{softmax}$ transforms the output weight (logits) into probabilities\n",
    "* $o$ is column vector that contains the probability of each category\n",
    "\n",
    "We will train this model via stochastic gradient descent by minimizing the negative log-likelihood of the data:\n",
    "$$\n",
    "    \\hat{W}, \\hat{b} = \\operatorname{argmin}_{W, b} \\sum_{x, y} - \\log p(y | x)\n",
    "$$\n",
    "Although this is a linear model, it classifies raw data without any manual feature extraction step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libs that we will use\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# To load the data we will use the script of Gaetan Marceau Caron\n",
    "# You can download it from the course webiste and move it to the same directory that contains this ipynb file\n",
    "import dataset_loader\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download mnist dataset \n",
    "if(\"mnist.pkl.gz\" not in os.listdir(\".\")):\n",
    "    !wget https://gerdes.fr/mnist.pkl.gz\n",
    "    # this looks down, so let's download it from Kim's website :)\n",
    "    #!wget http://deeplearning.net/data/mnist/mnist.pkl.gz\n",
    "\n",
    "# if you have it somewhere else, you can comment the lines above\n",
    "# and overwrite the path below\n",
    "mnist_path = \"./mnist.pkl.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the 3 splits\n",
    "train_data, dev_data, test_data = dataset_loader.load_mnist(mnist_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each dataset is a list with two elemets:\n",
    "* data[0] contains images\n",
    "* data[1] contains labels\n",
    "\n",
    "Data is stored as numpy.ndarray. You can use data[0][i] to retrieve image number i and data[1][i] to retrieve its label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_data))\n",
    "print(type(train_data[0]))\n",
    "print(type(train_data[1]))\n",
    "print(type(train_data[0][0]))\n",
    "print(type(train_data[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 5\n",
      "len  train_data = 2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 900\n",
    "label = train_data[1][index]\n",
    "picture = train_data[0][index]\n",
    "\n",
    "print(\"label: %i\" % label)\n",
    "plt.imshow(picture.reshape(28,28), cmap='Greys')\n",
    "print(\"len  train_data = {}\".format(len(train_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What are the characteristics of training data? (number of samples, dimension of input, number of labels)\n",
    "\n",
    "The documentation of ndarray class is available here: https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(train_data)=2 # the legnth of train_data=2 \n",
    "#train_data[0].shape= (50000, 784) #data[0] contain 50000 images and 784 pixel ofs columns \n",
    "#train_data[1].shape=(50000,) contains  50000 labels of the images \n",
    "#28*28=784pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDimDataset(data):\n",
    "    n_training = data[0].shape[0]# TODO\n",
    "    n_feature =data[0].shape[1]# TODO# TODO\n",
    "    n_label = data[1].shape[0]# TODO\n",
    "    return n_training, n_feature, n_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784, 50000)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getDimDataset(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Building functions\n",
    "\n",
    "We now need to build functions that are required for the neural network.\n",
    "$$\n",
    "    o = \\operatorname{softmax}(Wx + b) \\\\\n",
    "    L(x, y) = -\\log p(y | x) = -\\log o[y]\n",
    "$$\n",
    "\n",
    "Note that in numpy, operator @ is used for matrix multiplication while * is used for element-wise multiplication.\n",
    "The documentation for linear algebra in numpy is available here: https://docs.scipy.org/doc/numpy/reference/routines.linalg.html\n",
    "\n",
    "The first operation is the affine transformation $v = Wx + b$.\n",
    "To compute the gradient, it is often convenient to write the forward pass as $v[i] = b[i] + \\sum_j W[i, j] x[j]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input:\n",
    "# - x: vector of logits\n",
    "# Output\n",
    "# - vector of probabilities\n",
    "def softmax(x):\n",
    "    x=np.array(x)\n",
    "    b = x.max()\n",
    "    y = np.exp(x - b)\n",
    "    return y / y.sum()\n",
    "\n",
    "def derivee_sofmax(x):\n",
    "    return softmax(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input:\n",
    "# - W: projection matrix\n",
    "# - b: bias\n",
    "# - x: input features\n",
    "# Output:\n",
    "# - vector\n",
    "def affine_transform(W, b, x):\n",
    "    v = W@x+b # TODO\n",
    "    return v\n",
    "\n",
    "#supponsons que z = Ax + b.\n",
    "#g = grad_z loss (le gradient de la loss par rapport à z)\n",
    "#g_W = grad_W loss (le gradient de la loss par rapport à z)\n",
    "#g_b = grad_b loss (le gradient de la loss par rapport à z)\n",
    "\n",
    "# Input:\n",
    "# - W: projection matrix\n",
    "# - b: bias\n",
    "# - x: input features\n",
    "# - g: incoming gradient\n",
    "# Output:\n",
    "# - g_W: gradient wrt W\n",
    "# - g_b: gradient wrt b\n",
    "def backward_affine_transform(W, b, x, g):\n",
    "    #a=affine_transform(W,b,x)#  a.shape=(5,)=b.shape\n",
    "    #h=softmax(a) # h.shape=(5,) \n",
    "    #o=h # la sortie \n",
    "    #o_gold=g\n",
    "    #grad_h=g \n",
    "    #grad_h=g*derivee_sofmax(a)# # derivee loss expect to h or y . mutplication elt by elt\n",
    "    #grad_b=g # because  loss= L(o,y) +lambda*||W||²=0\n",
    "    #grad_w=np.dot( ( (derivee_sofmax(o)*(g)).reshape(-1,1) ) ,(x.reshape(-1,1).T) )  # because o_gold=g\n",
    "    #t.reshape(-1, 1)\n",
    "    #derivee_w_soft_max=np.dot(softmax(p).reshape(-1,1), x.reshape(-1,1).T)#its shape=(5,)\n",
    "    #grad_z_loss=g#its shape is (5,)\n",
    "    #g_W =np.dot(derivee_w_soft_max.T,g.reshape(-1,1));  #produit scalire ie multiply term par terme.                         # TODO\n",
    "    #g_b = softmax(p)*g# TODO \n",
    "    g_W=np.outer(g,x)# matrice de taille (g.shape[0],x.shape[0]\n",
    "    g_b=1*g            \n",
    "    return g_W, g_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell is a (too simple) test of affine_transform and backward_affine_transform.\n",
    "It should run without error if your implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.asarray([[ 0.63024213,  0.53679375, -0.92079597],\n",
    " [-0.1155045,   0.62780356, -0.67961305],\n",
    " [ 0.08465286, -0.06561815, -0.39778322],\n",
    " [ 0.8242268,   0.58907262, -0.52208052],\n",
    " [-0.43894227, -0.56993247,  0.09520727]])\n",
    "b = np.asarray([ 0.42706842,  0.69636598, -0.85611933, -0.08682553,  0.83160079])\n",
    "x = np.asarray([-0.32809223, -0.54751413,  0.81949319])\n",
    "#print(\" x.shape = {} \".format(x.shape))\n",
    "\n",
    "o_gold = np.asarray([-0.82819732, -0.16640748, -1.17394705, -1.10761496,  1.36568213])\n",
    "g = np.asarray([-0.08938868,  0.44083873, -0.2260743,  -0.96196726, -0.53428805])\n",
    "g_W_gold = np.asarray([[ 0.02932773,  0.04894156, -0.07325341],\n",
    " [-0.14463576, -0.24136543,  0.36126434],\n",
    " [ 0.07417322,  0.12377887, -0.18526635],\n",
    " [ 0.31561399,  0.52669067, -0.78832562],\n",
    " [ 0.17529576,  0.29253025, -0.43784542]])\n",
    "g_b_gold = np.asarray([-0.08938868,  0.44083873, -0.2260743,  -0.96196726, -0.53428805])\n",
    "\n",
    "\n",
    "# quick test of the forward pass\n",
    "o = affine_transform(W, b, x)\n",
    "if o.shape != o_gold.shape:\n",
    "    raise RuntimeError(\"Unexpected output dimension: got %s, expected %s\" % (str(o.shape), str(o_gold.shape)))\n",
    "if not np.allclose(o, o_gold):\n",
    "    raise RuntimeError(\"Output of the affine_transform function is incorrect\")\n",
    "    \n",
    "# quick test if the backward pass\n",
    "g_W, g_b = backward_affine_transform(W, b, x, g)\n",
    "if g_W.shape != g_W_gold.shape:\n",
    "        raise RuntimeError(\"Unexpected gradient dimension for W: got %s, expected %s\" % (str(g_W.shape), str(g_W_gold.shape)))\n",
    "if g_b.shape != g_b_gold.shape:\n",
    "        raise RuntimeError(\"Unexpected gradient dimension for b: got %s, expected %s\" % (str(g_b.shape), str(g_b_gold.shape)))\n",
    "if not np.allclose(g_W, g_W_gold):\n",
    "    raise RuntimeError(\"Gradient of W is incorrect\")\n",
    "if not np.allclose(g_b, g_b_gold):\n",
    "    raise RuntimeError(\"Gradient of b is incorrect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax function:\n",
    "$$\n",
    "     o = \\operatorname{softmax}(w)\n",
    "$$\n",
    "where $w$ is a vector of logits in $\\mathbb R$ and $o$ a vector of probabilities such that:\n",
    "$$\n",
    "    o[i] = \\frac{\\exp(w[i])}{\\sum_j \\exp(w[j])}\n",
    "$$\n",
    "We do not need to implement the backward for this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING:** is your implementation numerically stable?\n",
    "\n",
    "The $\\exp$ function results in computations that overflows (i.e. results in numbers that cannot be represented with floating point numbers).\n",
    "Therefore, it is always convenient to use the following trick to improve stability: https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Example for testing the numerical stability of softmax\n",
    "# It should return [1., 0. ,0.], not [nan, 0., 0.]\n",
    "z = [1000000,1,100]\n",
    "print(softmax(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: from the result of the cell above, what can you say about the softmax output, even when it is stable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just too simple test for the softmax function\n",
    "x = np.asarray([0.92424884, -0.92381088, -0.74666024, -0.87705478, -0.54797015])\n",
    "y_gold = np.asarray([0.57467369, 0.09053556, 0.10808233, 0.09486917, 0.13183925])\n",
    "\n",
    "y = softmax(x)\n",
    "if not np.allclose(y, y_gold):\n",
    "    raise RuntimeError(\"Output of the softmax function is incorrect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we build the loss function and its gradient for training the network.\n",
    "\n",
    "The loss function is the negative log-likelihood defined as:\n",
    "$$\n",
    "    \\mathcal L(x, gold) = -\\log \\frac{\\exp(x[gold])}{\\sum_j \\exp(x[j])} = -x[gold] + \\log \\sum_j \\exp(x[j])\n",
    "$$\n",
    "This function is also called the cross-entropy loss (in Pytorch, different names are used dependending if the inputs are probabilities or raw logits).\n",
    "\n",
    "Similarly to the softmax, we have to rely on the log-sum-exp trick to stabilize the computation: https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input:\n",
    "# - x: vector of logits\n",
    "# - gold: index of the gold class\n",
    "# Output:\n",
    "# - scalare equal to -log(softmax(x)[gold])\n",
    "def nll(x, gold):\n",
    "    # TODO\n",
    "    #res=-np.log(softmax(x)[gold])\n",
    "    res =(-x[gold] + np.log(np.exp(x).sum()))\n",
    "    return res\n",
    "# Input:\n",
    "# - x: vector of logits\n",
    "# - gold: index of the gold class\n",
    "# - gradient (scalar)\n",
    "# Output:\n",
    "# - gradient wrt x\n",
    "def backward_nll(x, gold, g):\n",
    "    #g_x =1*0/softmax(x)[gold] # TODO*\n",
    "    #its the derivative of nll_specte_to_x= (Jacobian_x z ).T*g\n",
    "    x1 = np.zeros(x.shape)# all eltments egal to zero \n",
    "    x1[gold] = 1 # the gold th elt =1\n",
    "    g_x = (softmax(x) - x1) * g\n",
    "    return g_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "x = np.asarray([-0.13590009, -0.83649656,  0.03130881,  0.42559402,  0.08488182])\n",
    "y_gold = 1.5695014420179738\n",
    "g_gold = np.asarray([ 0.17609875,  0.08739591, -0.79185107,  0.30875221,  0.2196042 ])\n",
    "\n",
    "y = nll(x, 2)\n",
    "g = backward_nll(x, 2, 1.)\n",
    "\n",
    "if not np.allclose(y, y_gold):\n",
    "    raise RuntimeError(\"Output is incorrect\")\n",
    "\n",
    "if g.shape != g_gold.shape:\n",
    "        raise RuntimeError(\"Unexpected gradient dimension: got %s, expected %s\" % (str(g.shape), str(g_gold.shape)))\n",
    "if not np.allclose(g, g_gold):\n",
    "    raise RuntimeError(\"Gradient is incorrect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code test the implementation of the gradient using finite-difference approximation, see: https://timvieira.github.io/blog/post/2017/04/21/how-to-test-gradient-implementations/\n",
    "\n",
    "Your implementation should pass this test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is python re-implementation of the test from the Dynet library\n",
    "# https://github.com/clab/dynet/blob/master/dynet/grad-check.cc\n",
    "\n",
    "def is_almost_equal(grad, computed_grad):\n",
    "    #print(grad, computed_grad)\n",
    "    f = abs(grad - computed_grad)\n",
    "    m = max(abs(grad), abs(computed_grad))\n",
    "\n",
    "    if f > 0.01 and m > 0.:\n",
    "        f /= m\n",
    "\n",
    "    if f > 0.01 or math.isnan(f):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def check_gradient(function, weights, true_grad, alpha = 1e-3):\n",
    "    # because input can be of any dimension,\n",
    "    # we build a view of the underlying data with the .shape(-1) method\n",
    "    # then we can access any element of the tensor as a elements of a list\n",
    "    # with a single dimension\n",
    "    weights_view = weights.reshape(-1)\n",
    "    true_grad_view = true_grad.reshape(-1)\n",
    "    for i in range(weights_view.shape[0]):\n",
    "        old = weights_view[i]\n",
    "\n",
    "        weights_view[i] = old - alpha\n",
    "        value_left = function(weights).reshape(-1)\n",
    "\n",
    "        weights_view[i] = old + alpha\n",
    "        value_right = function(weights).reshape(-1)\n",
    "\n",
    "        weights_view[i] = old\n",
    "        grad = (value_right - value_left) / (2. * alpha)\n",
    "\n",
    "        if not is_almost_equal(grad, true_grad_view[i]):\n",
    "            return False\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Test the affine transformation\n",
    "\n",
    "x = np.random.uniform(-1, 1, (5,))\n",
    "W = np.random.uniform(-1, 1, (3, 5))\n",
    "b = np.random.uniform(-1, 1, (3,))\n",
    "\n",
    "for i in range(3):\n",
    "    y = affine_transform(W, b, x)\n",
    "    g = np.zeros_like(y)\n",
    "    g[i] = 1.\n",
    "    g_W, _ = backward_affine_transform(W, b, x, g)\n",
    "    print(check_gradient(lambda W: affine_transform(W, b, x)[i], W, g_W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# test the negative likelihood loss\n",
    "\n",
    "x = np.random.uniform(-1, 1, (5,))\n",
    "\n",
    "for gold in range(5):\n",
    "    y = nll(x, gold)\n",
    "    g_y = backward_nll(x, gold, 1.)\n",
    "\n",
    "    print(check_gradient(lambda x: nll(x, gold), x, g_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Parameter initialization\n",
    "\n",
    "We are now going to build the function that will be used to initialize the parameters of the neural network before training.\n",
    "Note that for parameter initialization you must use **in-place** operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a random ndarray\n",
    "a = np.random.uniform(-1, 1, (5,))\n",
    "\n",
    "# this does not change the data of the ndarray created above!\n",
    "# it creates a new ndarray and replace the reference stored in a\n",
    "a = np.zeros((5, ))\n",
    "\n",
    "# this will change the underlying data of the ndarray that a points to\n",
    "a[:] = 0\n",
    "\n",
    "# similarly, this creates a new array and change the object pointed by a\n",
    "a = a + 1\n",
    "\n",
    "# while this change the underlying data of a\n",
    "a += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an affine transformation, it is common to:\n",
    "* initialize the bias to 0\n",
    "* initialize the projection matrix with Glorot initialization (also known as Xavier initialization)\n",
    "\n",
    "The formula for Glorot initialization can be found in equation 16 (page 5) of the original paper: http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_init(b):\n",
    "    b[:] = 0.\n",
    "\n",
    "def glorot_init(W):\n",
    "    size = math.sqrt(6.0/(W.shape[0] + W.shape[1]))\n",
    "    weights_init = np.random.uniform(-size, size, size=W.shape)\n",
    "    # TODO\n",
    "    return weights_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Building and training the neural network\n",
    "\n",
    "In our simple example, creating the neural network is simply instantiating the parameters $W$ and $b$.\n",
    "They must be ndarray object with the correct dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parameters(dim_input, dim_output):\n",
    "    W = np.zeros((dim_output,dim_input )) # output=W.T*input +b\n",
    "    b = np.zeros((dim_output, )) # b as the same dim for the output \n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recent success of deep learning is (partly) due to the ability to train very big neural networks.\n",
    "However, researchers became interested in building small neural networks to improve computational efficiency and memory usage.\n",
    "Therefore, we often want to compare neural networks by their number of parameters, i.e. the size of the memory required to store the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_n_parameters(W, b):\n",
    "    n = W.size + b.size # parameters in w   + pramertere in b TODO \n",
    "    print(\"Number of parameters: %i\" % (n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the neural network and print its number of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 7850\n"
     ]
    }
   ],
   "source": [
    "dim_input = getDimDataset(train_data)[1] #3 # TODO\n",
    "dim_output = np.unique(train_data[1]).shape[0]#5 # TODO\n",
    "W, b = create_parameters(dim_input, dim_output)\n",
    "print_n_parameters(W, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the training loop!\n",
    "\n",
    "The training loop should be structured as follows:\n",
    "* we do **epochs** over the data, i.e. one epoch is one loop over the dataset\n",
    "* at each epoch, we first loop over the data and update the network parameters with respect to the loss gradient\n",
    "* at the end of each epoch, we evaluate the network on the dev dataset\n",
    "* after all epochs are done, we evaluate our network on the test dataset and compare its performance with the performance on dev\n",
    "\n",
    "During training, it is useful to print the following information:\n",
    "* the mean loss over the epoch: it should be decreasing!\n",
    "* the accuracy on the dev set: it should be increasing!\n",
    "* the accuracy on the train set: it shoud be increasing!\n",
    "\n",
    "If you observe a decreasing loss (+increasing accuracy on test data) but decreasing accuracy on dev data, your network is overfitting!\n",
    "\n",
    "Once you have build **and tested** this a simple training loop, you should introduce the following improvements:\n",
    "* instead of evaluating on dev after each loop on the training data, you can also evaluate on dev n times per epoch\n",
    "* shuffle the data before each epoch\n",
    "* instead of memorizing the parameters of the last epoch only, you should have a copy of the parameters that produced the best value on dev data during training and evaluate on test with those instead of the parameters after the last epoch\n",
    "* learning rate decay: if you do not observe improvement on dev, you can try to reduce the step size\n",
    "\n",
    "After you conducted (successful?) experiments, you should write a report with results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataa=0.1*np.ones((3,5))\n",
    "boolean =isinstance(train_data[0],np.ndarray)\n",
    "#boolean =isinstance(2,int)\n",
    "#help(isinstance)\n",
    "boolean\n",
    "arr=np.array([\"ddd\"])\n",
    "b1=type(arr)==np.ndarray\n",
    "arr1=np.zeros_like(dataa)\n",
    "arr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before training, we initialize the parameters of the network\n",
    "zero_init(b)\n",
    "W=glorot_init(W)\n",
    "\n",
    "n_epochs = 5 # number of epochs\n",
    "step = 0.01 # step size for gradient updates\n",
    "n=0\n",
    "for epoch in range(n_epochs):\n",
    "    # TODO\n",
    "    # ...\n",
    "    theta = [W,b]\n",
    "    best_acc = 0\n",
    "    acc_train = []\n",
    "    acc_dev = []\n",
    "    X_train = train_data[0]\n",
    "    Y_train = train_data[1]\n",
    "    for i in range(0,getDimDataset(train_data)[0]):\n",
    "        x_row = X_train[i]\n",
    "        gold = Y_train[i]\n",
    "        \n",
    "        #forward pass\n",
    "        x_row_tranformed = affine_transform(W, b, x_row)\n",
    "        \n",
    "        y_gold = np.argmax(  x_row_tranformed)\n",
    "        if y_gold == gold:\n",
    "            n += 1\n",
    "        \n",
    "        #training\n",
    "        g_x = backward_nll( x_row_tranformed, gold, 1.)\n",
    "        \n",
    "        \n",
    "        g_W, g_b = backward_affine_transform(W, b, x_row, g_x)\n",
    "        W = W - step*g_W\n",
    "        b = b - step*g_b\n",
    "        \n",
    "    \n",
    "# Test evaluation\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
